# ğŸš€ Custom Tokenizer â€” BPE & WordPiece (Transformer-Compatible)

## ğŸ“˜ Overview
This project implements a **custom tokenizer framework** written entirely in **Python**, featuring both **Byte Pair Encoding (BPE)** and **WordPiece** algorithms.  
It includes a **Transformer-compatible tokenizer wrapper** for direct use in NLP models such as BERT or any custom Transformer architecture.

---

## âš™ï¸ Features
- ğŸ§© **BPE Tokenizer:** Frequency-based subword merging  
- ğŸ§  **WordPiece Tokenizer:** Probabilistic subword selection (BERT-style)  
- ğŸ”„ **Transformer Integration:** Model-ready encoding (input IDs, masks, etc.)  
- ğŸ’¾ **Custom Vocabulary Learning:** Train, save, and load vocabularies  
- ğŸ§° **Pure Python:** Minimal dependencies, simple to extend

---

## ğŸ§  Theory
- **Byte Pair Encoding (BPE):** Iteratively merges the most frequent symbol pairs in a corpus to form subword units.  
- **WordPiece:** Similar to BPE but selects merges based on maximum likelihood estimation. Commonly used in BERT and related models.
